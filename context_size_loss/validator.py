#!/usr/bin/env python3
"""
Simplified Validation Module for Context Size Loss Experiment

This module provides fast, simple validation of sprintfâ†’snprintf conversions
using golden answers generated by gemini-2.5-pro and edit distance comparison.

The validation approach:
1. Generate golden answers using gemini-2.5-pro with batch size 1
2. Compare experimental results to golden answers using normalized edit distance
3. Use 80% similarity threshold for correctness determination

Author: AI Assistant
Date: 2025
"""

import json
import logging
import time
import dataclasses
import pathlib
import typing

# Configure logging
logger = logging.getLogger(__name__)


@dataclasses.dataclass
class ValidationResult:
    """Simple validation result for a single conversion."""
    
    # Core results
    is_correct: bool
    similarity_score: float  # 0.0 to 1.0
    edit_distance: int
    threshold: float
    
    # Additional metrics
    generated_length: int
    golden_length: int
    validation_time_ms: float
    
    # Optional details
    error_message: typing.Optional[str] = None
    suggestions: typing.Optional[typing.List[str]] = None
    
    @classmethod
    def create_no_golden_answer_error(cls) -> 'ValidationResult':
        """Create a validation result for missing golden answer."""
        return cls(
            is_correct=False,
            similarity_score=0.0,
            edit_distance=0,
            threshold=0.8,
            generated_length=0,
            golden_length=0,
            validation_time_ms=0.0,
            error_message="No golden answer available for comparison"
        )
    
    @classmethod
    def create_timeout_error(cls) -> 'ValidationResult':
        """Create a validation result for timeout."""
        return cls(
            is_correct=False,
            similarity_score=0.0,
            edit_distance=0,
            threshold=0.8,
            generated_length=0,
            golden_length=0,
            validation_time_ms=0.0,
            error_message="Validation timed out"
        )


def levenshtein_distance(s1: str, s2: str) -> int:
    """
    Calculate the Levenshtein distance between two strings.
    
    Args:
        s1: First string
        s2: Second string
        
    Returns:
        Edit distance between the strings
    """
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = list(range(len(s2) + 1))
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]


def normalize_code(code: str) -> str:
    """
    Normalize code for comparison by removing extra whitespace and standardizing format.
    
    Args:
        code: Code string to normalize
        
    Returns:
        Normalized code string
    """
    # Remove extra whitespace and normalize line endings
    lines = [line.strip() for line in code.split('\n')]
    # Remove empty lines at start and end
    while lines and not lines[0]:
        lines.pop(0)
    while lines and not lines[-1]:
        lines.pop()
    return '\n'.join(lines)


def validate_with_edit_distance(generated_code: str, golden_code: str, 
                               threshold: float = 0.8) -> ValidationResult:
    """
    Validate generated code against golden answer using edit distance.
    
    Args:
        generated_code: Code generated by experiment
        golden_code: Golden reference code
        threshold: Similarity threshold (default 0.8 = 80%)
        
    Returns:
        ValidationResult with similarity score and correctness
    """
    start_time = time.time()
    
    # Normalize both codes for fair comparison
    norm_generated = normalize_code(generated_code)
    norm_golden = normalize_code(golden_code)
    
    # Calculate normalized edit distance
    distance = levenshtein_distance(norm_generated, norm_golden)
    max_length = max(len(norm_generated), len(norm_golden))
    similarity = 1.0 - (distance / max_length) if max_length > 0 else 1.0
    
    # Determine correctness
    is_correct = similarity >= threshold
    
    validation_time = (time.time() - start_time) * 1000  # Convert to milliseconds
    
    return ValidationResult(
        is_correct=is_correct,
        similarity_score=similarity,
        edit_distance=distance,
        threshold=threshold,
        generated_length=len(norm_generated),
        golden_length=len(norm_golden),
        validation_time_ms=validation_time
    )


class GoldenAnswerManager:
    """Manages golden answers for validation."""
    
    def __init__(self, golden_answers_file: str):
        self.golden_answers_file = pathlib.Path(golden_answers_file)
        self.golden_answers = self.load_golden_answers()
    
    def load_golden_answers(self) -> typing.Dict[str, typing.Dict[str, typing.List[str]]]:
        """Load golden answers from file."""
        if not self.golden_answers_file.exists():
            logger.info(f"Golden answers file {self.golden_answers_file} does not exist, starting with empty dict")
            return {}
        
        try:
            with open(self.golden_answers_file, 'r') as f:
                data = json.load(f)
                logger.info(f"Loaded {len(data)} golden answers from {self.golden_answers_file}")
                return data
        except (json.JSONDecodeError, IOError) as e:
            logger.error(f"Error loading golden answers: {e}")
            return {}
    
    def save_golden_answers(self, golden_answers: typing.Optional[typing.Dict[str, typing.Dict[str, typing.List[str]]]] = None):
        """Save golden answers to file."""
        if golden_answers is None:
            golden_answers = self.golden_answers
        
        # Ensure directory exists
        self.golden_answers_file.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            with open(self.golden_answers_file, 'w') as f:
                json.dump(golden_answers, f, indent=2)
            logger.info(f"Saved {len(golden_answers)} golden answers to {self.golden_answers_file}")
        except IOError as e:
            logger.error(f"Error saving golden answers: {e}")
    
    def has_golden_answer(self, snippet_id: str) -> bool:
        """Check if golden answer exists for a snippet."""
        return snippet_id in self.golden_answers
    
    def get_golden_answer(self, snippet_id: str) -> typing.Optional[str]:
        """Get golden answer for a snippet as a single string."""
        if snippet_id not in self.golden_answers:
            return None
        
        snippet_data = self.golden_answers[snippet_id]
        if 'golden_answer' in snippet_data:
            return '\n'.join(snippet_data['golden_answer'])
        return None
    
    def get_original_content(self, snippet_id: str) -> typing.Optional[str]:
        """Get original content for a snippet as a single string."""
        if snippet_id not in self.golden_answers:
            return None
        
        snippet_data = self.golden_answers[snippet_id]
        if 'original_content' in snippet_data:
            return '\n'.join(snippet_data['original_content'])
        return None
    
    def add_golden_answer(self, snippet_id: str, original_content: str, golden_code: str):
        """Add a golden answer for a snippet with both original and converted content."""
        # Convert strings to lists of lines
        original_lines = original_content.split('\n')
        golden_lines = golden_code.split('\n')
        
        self.golden_answers[snippet_id] = {
            'original_content': original_lines,
            'golden_answer': golden_lines
        }
        logger.debug(f"Added golden answer for snippet {snippet_id}")
    
    def get_missing_snippets(self, snippet_ids: typing.List[str]) -> typing.List[str]:
        """Get list of snippet IDs that don't have golden answers."""
        return [snippet_id for snippet_id in snippet_ids 
                if not self.has_golden_answer(snippet_id)]


class ExperimentValidator:
    """Simplified validator for the experiment."""
    
    def __init__(self, golden_answers_file: str, similarity_threshold: float = 0.8):
        self.golden_manager = GoldenAnswerManager(golden_answers_file)
        self.similarity_threshold = similarity_threshold
        logger.info(f"Initialized validator with threshold {similarity_threshold}")
    
    def validate_single(self, snippet_id: str, generated_code: str) -> ValidationResult:
        """Validate a single generated code against its golden answer."""
        
        if not self.golden_manager.has_golden_answer(snippet_id):
            return ValidationResult.create_no_golden_answer_error()
        
        golden_code = self.golden_manager.get_golden_answer(snippet_id)
        return validate_with_edit_distance(
            generated_code, 
            golden_code, 
            self.similarity_threshold
        )
    
    def validate_batch(self, snippet_ids: typing.List[str], 
                      generated_codes: typing.List[str]) -> typing.List[ValidationResult]:
        """Validate a batch of generated codes against golden answers."""
        
        if len(snippet_ids) != len(generated_codes):
            raise ValueError("snippet_ids and generated_codes must have the same length")
        
        results = []
        for snippet_id, generated_code in zip(snippet_ids, generated_codes):
            result = self.validate_single(snippet_id, generated_code)
            results.append(result)
        
        logger.info(f"Validated {len(results)} conversions")
        return results
    
    def calculate_batch_metrics(self, results: typing.List[ValidationResult]) -> typing.Dict[str, float]:
        """Calculate metrics for a batch of validation results."""
        
        if not results:
            return {
                "total_conversions": 0,
                "correct_conversions": 0,
                "success_rate": 0.0,
                "average_similarity": 0.0,
                "threshold": self.similarity_threshold
            }
        
        total = len(results)
        correct = sum(1 for r in results if r.is_correct)
        avg_similarity = sum(r.similarity_score for r in results) / total
        
        # Calculate additional metrics
        valid_results = [r for r in results if r.error_message is None]
        avg_validation_time = sum(r.validation_time_ms for r in valid_results) / len(valid_results) if valid_results else 0.0
        
        metrics = {
            "total_conversions": total,
            "correct_conversions": correct,
            "success_rate": correct / total,
            "average_similarity": avg_similarity,
            "threshold": self.similarity_threshold,
            "average_validation_time_ms": avg_validation_time,
            "error_count": sum(1 for r in results if r.error_message is not None)
        }
        
        logger.info(f"Batch metrics: {correct}/{total} correct ({metrics['success_rate']:.2%}), "
                   f"avg similarity: {avg_similarity:.3f}")
        
        return metrics
    
    def get_validation_summary(self, results: typing.List[ValidationResult]) -> str:
        """Get a human-readable summary of validation results."""
        
        metrics = self.calculate_batch_metrics(results)
        
        summary = f"""
Validation Summary:
==================
Total conversions: {metrics['total_conversions']}
Correct conversions: {metrics['correct_conversions']}
Success rate: {metrics['success_rate']:.2%}
Average similarity: {metrics['average_similarity']:.3f}
Threshold: {metrics['threshold']:.1%}
Average validation time: {metrics['average_validation_time_ms']:.2f}ms
Errors: {metrics['error_count']}
"""
        
        return summary.strip()


def generate_golden_answers(snippets: typing.List, api_key: str, 
                          golden_answers_file: str = "golden_answers.json") -> typing.Dict[str, typing.Dict[str, typing.List[str]]]:
    """
    Generate golden answers using gemini-2.5-pro with batch size 1.
    
    This is a placeholder function - in practice, you would integrate with
    the actual Gemini API to generate the golden answers.
    
    Args:
        snippets: List of code snippets to convert
        api_key: Gemini API key
        golden_answers_file: File to save golden answers
        
    Returns:
        Dictionary mapping snippet ID to golden answer data with original and converted content
    """
    logger.info(f"Generating golden answers for {len(snippets)} snippets...")
    
    # This is a placeholder - in practice, you would:
    # 1. Call Gemini API with each snippet individually (batch size 1)
    # 2. Use gemini-2.5-pro model for highest quality
    # 3. Store results in golden_answers dictionary
    
    golden_answers = {}
    
    # Placeholder: For now, just create empty entries
    # In practice, replace this with actual API calls
    for i, snippet in enumerate(snippets):
        snippet_id = getattr(snippet, 'id', f"snippet_{i}")
        # Get original content
        original_content = getattr(snippet, 'content', '') or getattr(snippet, 'get_full_content', lambda: '')()
        # Placeholder golden answer - replace with actual API call
        golden_code = f"// Golden answer for {snippet_id} - TODO: Generate with Gemini API"
        
        # Convert to line-based format
        golden_answers[snippet_id] = {
            'original_content': original_content.split('\n'),
            'golden_answer': golden_code.split('\n')
        }
    
    # Save golden answers
    manager = GoldenAnswerManager(golden_answers_file)
    manager.golden_answers = golden_answers
    manager.save_golden_answers()
    
    logger.info(f"Generated {len(golden_answers)} golden answers")
    return golden_answers


# Example usage and testing
if __name__ == "__main__":
    # Test the validation system
    print("Testing validation system...")
    
    # Test edit distance calculation
    test_generated = "snprintf(buf, sizeof(buf), \"%s\", str);"
    test_golden = "snprintf(buf, sizeof(buf), \"%s\", str);"
    
    result = validate_with_edit_distance(test_generated, test_golden)
    print(f"Test result: {result}")
    
    # Test with different code
    test_generated2 = "sprintf(buf, \"%s\", str);"  # Wrong - still sprintf
    result2 = validate_with_edit_distance(test_generated2, test_golden)
    print(f"Test result 2: {result2}")
    
    print("Validation system test completed.")
